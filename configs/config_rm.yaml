oasst-rm-1-pythia-1.4b:
  is_reward_model: true
  pooling: last
  datasets:
    - oasst_export:
        lang: "en,es,de,fr"
        hf_dataset_name: OpenAssistant/oasst1
        val_split: 0.1
  use_custom_sampler: true
  sort_by_length: false
  model_name: andreaskoepf/pythia-1.4b-gpt4all-pretrain
  learning_rate: 8e-6
  residual_dropout: 0.01
  weight_decay: 0.0
  dtype: float32
  max_length: 2048
  use_flash_attention: true
  warmup_steps: 50
  gradient_accumulation_steps: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 5
  num_train_epochs: 2
  eval_steps: 500
  save_steps: 1000